{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dee8ba7fca03496f987b8b1eeecd3861": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_28e3e5ad75bc494cb7eba7bc280c4ce9",
              "IPY_MODEL_d767adf3427d4133a6dae84f6d0b5eb9",
              "IPY_MODEL_33729c80247047d18ca7e99264213ec4"
            ],
            "layout": "IPY_MODEL_3571c65854724d89b4c9cbf99ef7fe5c"
          }
        },
        "28e3e5ad75bc494cb7eba7bc280c4ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da1920ae1e864622939406df2a1e0d30",
            "placeholder": "​",
            "style": "IPY_MODEL_cbec20b051874276b869578cf78b82ff",
            "value": "Upload 2 LFS files: 100%"
          }
        },
        "d767adf3427d4133a6dae84f6d0b5eb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58d7becf721340d0bcb49bb7f3f18fbc",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ff78cedcd6df40a2b60662f0ad856631",
            "value": 2
          }
        },
        "33729c80247047d18ca7e99264213ec4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a80070b61d6441a8a14e3212e7245f99",
            "placeholder": "​",
            "style": "IPY_MODEL_627c884ddd3242b89f090f53a82d081b",
            "value": " 2/2 [03:18&lt;00:00, 86.34s/it]"
          }
        },
        "3571c65854724d89b4c9cbf99ef7fe5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da1920ae1e864622939406df2a1e0d30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbec20b051874276b869578cf78b82ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58d7becf721340d0bcb49bb7f3f18fbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff78cedcd6df40a2b60662f0ad856631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a80070b61d6441a8a14e3212e7245f99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "627c884ddd3242b89f090f53a82d081b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2d5758ff64d4ea6a4085879d82b6104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c888ac93c5b7432cb03d01cf597d50b0",
              "IPY_MODEL_d10cc3977bf846e28c1e9f80ee51283c",
              "IPY_MODEL_2872e1a494a9483eaebf1e7e80e2f3c3"
            ],
            "layout": "IPY_MODEL_b0042b1a0e3c4c018ed5a3fc925efa64"
          }
        },
        "c888ac93c5b7432cb03d01cf597d50b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86b717eb6efc4990be6f32f827482b76",
            "placeholder": "​",
            "style": "IPY_MODEL_094bee69954f47a9a1b1445970682c2c",
            "value": "mistral-7b-instruct-v0.2.Q4_K_M.gguf: 100%"
          }
        },
        "d10cc3977bf846e28c1e9f80ee51283c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad71f8a66f04f9a9a1572e2ef18cb0d",
            "max": 4369375808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dff26bc4414b4f7f895a0e88d023f9f2",
            "value": 4369375808
          }
        },
        "2872e1a494a9483eaebf1e7e80e2f3c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47387728e92b499e9319e4c5e8af9289",
            "placeholder": "​",
            "style": "IPY_MODEL_df93ca3666bc4db28470a8aa5f0b0f44",
            "value": " 4.37G/4.37G [02:51&lt;00:00, 30.0MB/s]"
          }
        },
        "b0042b1a0e3c4c018ed5a3fc925efa64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b717eb6efc4990be6f32f827482b76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "094bee69954f47a9a1b1445970682c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ad71f8a66f04f9a9a1572e2ef18cb0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dff26bc4414b4f7f895a0e88d023f9f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47387728e92b499e9319e4c5e8af9289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df93ca3666bc4db28470a8aa5f0b0f44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20f3264f1c1e4c0db616a384ca025ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eda9512d149842228fb832507d56e5f6",
              "IPY_MODEL_98caacd966b34924b25863f8ae08de3a",
              "IPY_MODEL_43ca42dc4952486ca79f4239e4e7520a"
            ],
            "layout": "IPY_MODEL_360c9987ca4247e5a988f65e2aa6eff1"
          }
        },
        "eda9512d149842228fb832507d56e5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d06b09c5e9e45cbadbd2b75e6e5a734",
            "placeholder": "​",
            "style": "IPY_MODEL_ed5ae2b26b40472980222d3115a29cd4",
            "value": "mistral-7b-instruct-v0.2.Q5_K_M.gguf: 100%"
          }
        },
        "98caacd966b34924b25863f8ae08de3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40c765e4b7d446d2a8d4af247169f3f8",
            "max": 5132345920,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0206763b677d4cb19ad184536e597fd7",
            "value": 5132345920
          }
        },
        "43ca42dc4952486ca79f4239e4e7520a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7c73a16917d49b9845318e2c49e2486",
            "placeholder": "​",
            "style": "IPY_MODEL_dab02bde5eda4d94a97f9a71cb1895cb",
            "value": " 5.13G/5.13G [03:17&lt;00:00, 11.9MB/s]"
          }
        },
        "360c9987ca4247e5a988f65e2aa6eff1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d06b09c5e9e45cbadbd2b75e6e5a734": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed5ae2b26b40472980222d3115a29cd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40c765e4b7d446d2a8d4af247169f3f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0206763b677d4cb19ad184536e597fd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7c73a16917d49b9845318e2c49e2486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dab02bde5eda4d94a97f9a71cb1895cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wenqiglantz/llmops/blob/main/Quantize_Mistral_7B_Instruct_v0_2_using_GGUF_and_llama_cpp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantize Mistral-7B-Instruct-v0.2 using GGUF and llama.cpp\n",
        "\n",
        "This notebook demonstrates how to quantize `Mistral-7B-Instruct-v0.2` using GGUF and llama.cpp.\n",
        "\n",
        "* `MODEL_ID`: `mistralai/Mistral-7B-Instruct-v0.2`\n",
        "* `QUANTIZATION_METHOD`: The quantization method to use.\n",
        "    - Q5_K_M: 5-bit, recommended, low quality loss.\n",
        "    - Q4_K_M: 4-bit, recommended, offers balanced quality.\n",
        "\n",
        "\n",
        "A big shout out to Maxime Labonne for his great work on quantizing Llama models through his blog post https://medium.com/towards-data-science/quantize-llama-models-with-ggml-and-llama-cpp-3612dfbcc172."
      ],
      "metadata": {
        "id": "8y_Rk94LzG7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantize model"
      ],
      "metadata": {
        "id": "TNItN0MS7C5K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log into Hugging Face\n",
        "\n",
        "Since we will be downloading the base model `mistralai/Mistral-7B-Instruct-v0.2` from Hugging Face hub and uploading our quantized models back to Hugging Face hub, let's log into Hugging Face first.  I store my Hugging Face token in the secrets tab to the left.  The benefit of storing my token in this secrets tab is that I don't expose the token in my notebook, and I can reuse this secrets configuration for all my Colab notebooks."
      ],
      "metadata": {
        "id": "2M5gVSa6sW73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "username = api.whoami()['name']\n",
        "print(username)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsVSBVQ2XmAQ",
        "outputId": "ae184d1a-e840-423b-f7a4-320d82ddc4ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wenqiglantz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install llama.cpp\n",
        "\n",
        "We need llama.cpp to quantize our base model, so let's install it."
      ],
      "metadata": {
        "id": "wprFbubXs84I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && git pull && make clean && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tJ3T464cYhnt",
        "outputId": "217c7c17-8882-4f25-b0a0-b75dbd141b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 15421, done.\u001b[K\n",
            "remote: Counting objects: 100% (5417/5417), done.\u001b[K\n",
            "remote: Compressing objects: 100% (303/303), done.\u001b[K\n",
            "remote: Total 15421 (delta 5284), reused 5133 (delta 5114), pack-reused 10004\u001b[K\n",
            "Receiving objects: 100% (15421/15421), 17.96 MiB | 22.03 MiB/s, done.\n",
            "Resolving deltas: 100% (10779/10779), done.\n",
            "Already up to date.\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS:  \n",
            "I LDFLAGS:    \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "rm -vrf *.o tests/*.o *.so *.dll benchmark-matmult common/build-info.cpp *.dot *.gcno tests/*.gcno *.gcda tests/*.gcda *.gcov tests/*.gcov lcov-report gcovr-report main quantize quantize-stats perplexity embedding vdot q8dot train-text-from-scratch convert-llama2c-to-ggml simple batched batched-bench save-load-state server gguf llama-bench libllava.a llava-cli baby-llama beam-search speculative infill tokenize benchmark-matmult parallel finetune export-lora lookahead lookup passkey tests/test-c.o metal tests/test-llama-grammar tests/test-grammar-parser tests/test-double-float tests/test-grad0 tests/test-opt tests/test-quantize-fns tests/test-quantize-perf tests/test-sampling tests/test-tokenizer-0-llama tests/test-tokenizer-0-falcon tests/test-tokenizer-1-llama tests/test-tokenizer-1-bpe tests/test-rope tests/test-backend-ops\n",
            "I llama.cpp build info: \n",
            "I UNAME_S:   Linux\n",
            "I UNAME_P:   x86_64\n",
            "I UNAME_M:   x86_64\n",
            "I CFLAGS:    -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion \n",
            "I CXXFLAGS:  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi\n",
            "I NVCCFLAGS: -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128 \n",
            "I LDFLAGS:   -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "I CC:        cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "I CXX:       g++ (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\n",
            "\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml.c -o ggml.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c llama.cpp -o llama.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/common.cpp -o common.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/sampling.cpp -o sampling.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/grammar-parser.cpp -o grammar-parser.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/build-info.cpp -o build-info.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/console.cpp -o console.o\n",
            "nvcc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -use_fast_math --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -DGGML_CUDA_PEER_MAX_BATCH_SIZE=128  -Wno-pedantic -Xcompiler \"-Wno-array-bounds -Wno-format-truncation -Wextra-semi\" -c ggml-cuda.cu -o ggml-cuda.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-alloc.c -o ggml-alloc.o\n",
            "cc  -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion    -c ggml-backend.c -o ggml-backend.o\n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion     -c ggml-quants.c -o ggml-quants.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/main/main.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o main -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "\n",
            "====  Run ./main -h for help.  ====\n",
            "\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize/quantize.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/quantize-stats/quantize-stats.cpp build-info.o ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o quantize-stats -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/perplexity/perplexity.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o perplexity -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/embedding/embedding.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o embedding -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/vdot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o vdot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi pocs/vdot/q8dot.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o q8dot -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -c common/train.cpp -o train.o\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o train-text-from-scratch -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o convert-llama2c-to-ggml -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/simple/simple.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o simple -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched/batched.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/batched-bench/batched-bench.cpp build-info.o ggml.o llama.o common.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o batched-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o save-load-state -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -Iexamples/server examples/server/server.cpp examples/llava/clip.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o server -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib   -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/gguf/gguf.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o gguf -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llama-bench -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi -static -fPIC -c examples/llava/llava.cpp -o libllava.a -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/llava/llava-cli.cpp examples/llava/clip.cpp examples/llava/llava.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o llava-cli -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib  -Wno-cast-qual\n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o baby-llama -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/beam-search/beam-search.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o beam-search -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/speculative/speculative.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o speculative -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/infill/infill.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o console.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o infill -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/tokenize/tokenize.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o tokenize -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/benchmark/benchmark-matmult.cpp build-info.o ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o benchmark-matmult -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/parallel/parallel.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o parallel -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/finetune/finetune.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o train.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o finetune -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/export-lora/export-lora.cpp ggml.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o export-lora -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookahead/lookahead.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookahead -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/lookup/lookup.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o lookup -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "g++ -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c++11 -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wmissing-declarations -Wmissing-noreturn -pthread  -march=native -mtune=native -Wno-array-bounds -Wno-format-truncation -Wextra-semi examples/passkey/passkey.cpp ggml.o llama.o common.o sampling.o grammar-parser.o build-info.o ggml-cuda.o ggml-alloc.o ggml-backend.o ggml-quants.o -o passkey -lcuda -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib -L/usr/local/cuda/targets/aarch64-linux/lib -L/usr/lib/wsl/lib \n",
            "cc -I. -Icommon -D_XOPEN_SOURCE=600 -D_GNU_SOURCE -DNDEBUG -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -I/usr/local/cuda/targets/aarch64-linux/include  -std=c11   -fPIC -O3 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration -pthread -march=native -mtune=native -Wdouble-promotion  -c tests/test-c.c -o tests/test-c.o\n",
            "Collecting numpy~=1.24.4 (from -r llama.cpp/./requirements/requirements-convert.txt (line 1))\n",
            "  Downloading numpy-1.24.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece~=0.1.98 (from -r llama.cpp/./requirements/requirements-convert.txt (line 2))\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.35.2 in /usr/local/lib/python3.10/dist-packages (from -r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.35.2)\n",
            "Collecting gguf>=0.1.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 4))\n",
            "  Downloading gguf-0.6.0-py3-none-any.whl (23 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.0 (from -r llama.cpp/./requirements/requirements-convert.txt (line 5))\n",
            "  Downloading protobuf-4.25.1-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=2.1.1 (from -r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.20.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.18.1 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (20.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/20.5 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5.0.0,>=4.35.2->-r llama.cpp/./requirements/requirements-convert.txt (line 3)) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch~=2.1.1->-r llama.cpp/./requirements/requirements-convert-hf-to-gguf.txt (line 2)) (1.3.0)\n",
            "Installing collected packages: sentencepiece, protobuf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, gguf, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.1.0+cu121\n",
            "    Uninstalling torch-2.1.0+cu121:\n",
            "      Successfully uninstalled torch-2.1.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "tensorboard 2.15.1 requires protobuf<4.24,>=3.19.6, but you have protobuf 4.25.1 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.25.1 which is incompatible.\n",
            "torchaudio 2.1.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchdata 0.7.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchtext 0.16.0 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\n",
            "torchvision 0.16.0+cu121 requires torch==2.1.0, but you have torch 2.1.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gguf-0.6.0 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 protobuf-4.25.1 sentencepiece-0.1.99 torch-2.1.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download, convert, and quantize the base model\n",
        "\n",
        "We first download the base model, then convert it to fp16, finally quantize it into both 5-bit and 4-bit models."
      ],
      "metadata": {
        "id": "lt860NZztY4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "MODEL_ID = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "QUANTIZATION_METHODS = [\"q5_k_m\", \"q4_k_m\"]\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = MODEL_ID.split('/')[-1]\n",
        "print(MODEL_NAME)\n",
        "\n",
        "# Download model\n",
        "!git lfs install\n",
        "!git clone https://{username}:{HF_TOKEN}@huggingface.co/{MODEL_ID}\n",
        "\n",
        "# Convert to fp16\n",
        "fp16 = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.fp16.bin\"\n",
        "!python llama.cpp/convert.py {MODEL_NAME} --outtype f16 --outfile {fp16}\n",
        "\n",
        "# Quantize the model for each method in the QUANTIZATION_METHODS list\n",
        "for method in QUANTIZATION_METHODS:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/quantize {fp16} {qtype} {method}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fD24jJxq7t3k",
        "outputId": "836145ed-f752-4f2c-eaff-80b4590cf964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mistral-7B-Instruct-v0.2\n",
            "Git LFS initialized.\n",
            "Cloning into 'Mistral-7B-Instruct-v0.2'...\n",
            "remote: Enumerating objects: 44, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (42/42), done.\u001b[K\n",
            "remote: Total 44 (delta 17), reused 0 (delta 0), pack-reused 2\u001b[K\n",
            "Unpacking objects: 100% (44/44), 469.65 KiB | 1.05 MiB/s, done.\n",
            "Filtering content: 100% (7/7), 3.46 GiB | 12.07 MiB/s, done.\n",
            "Encountered 6 file(s) that may not have been copied correctly on Windows:\n",
            "\tpytorch_model-00002-of-00003.bin\n",
            "\tpytorch_model-00003-of-00003.bin\n",
            "\tpytorch_model-00001-of-00003.bin\n",
            "\tmodel-00003-of-00003.safetensors\n",
            "\tmodel-00001-of-00003.safetensors\n",
            "\tmodel-00002-of-00003.safetensors\n",
            "\n",
            "See: `git lfs help smudge` for more details.\n",
            "Loading model file Mistral-7B-Instruct-v0.2/model-00001-of-00003.safetensors\n",
            "Loading model file Mistral-7B-Instruct-v0.2/model-00001-of-00003.safetensors\n",
            "Loading model file Mistral-7B-Instruct-v0.2/model-00002-of-00003.safetensors\n",
            "Loading model file Mistral-7B-Instruct-v0.2/model-00003-of-00003.safetensors\n",
            "params = Params(n_vocab=32000, n_embd=4096, n_layer=32, n_ctx=32768, n_ff=14336, n_head=32, n_head_kv=8, n_experts=None, n_experts_used=None, f_norm_eps=1e-05, rope_scaling_type=None, f_rope_freq_base=1000000.0, f_rope_scale=None, n_orig_ctx=None, rope_finetuned=None, ftype=<GGMLFileType.MostlyF16: 1>, path_model=PosixPath('Mistral-7B-Instruct-v0.2'))\n",
            "32000 32000\n",
            "Vocab info: <VocabLoader with 32000 base tokens and 0 added tokens>\n",
            "Special vocab info: <SpecialVocab with 58980 merges, special tokens {'bos': 1, 'eos': 2, 'unk': 0}, add special tokens {'bos': True, 'eos': False}>\n",
            "Permuting layer 0\n",
            "Permuting layer 1\n",
            "Permuting layer 2\n",
            "Permuting layer 3\n",
            "Permuting layer 4\n",
            "Permuting layer 5\n",
            "Permuting layer 6\n",
            "Permuting layer 7\n",
            "Permuting layer 8\n",
            "Permuting layer 9\n",
            "Permuting layer 10\n",
            "Permuting layer 11\n",
            "Permuting layer 12\n",
            "Permuting layer 13\n",
            "Permuting layer 14\n",
            "Permuting layer 15\n",
            "Permuting layer 16\n",
            "Permuting layer 17\n",
            "Permuting layer 18\n",
            "Permuting layer 19\n",
            "Permuting layer 20\n",
            "Permuting layer 21\n",
            "Permuting layer 22\n",
            "Permuting layer 23\n",
            "Permuting layer 24\n",
            "Permuting layer 25\n",
            "Permuting layer 26\n",
            "Permuting layer 27\n",
            "Permuting layer 28\n",
            "Permuting layer 29\n",
            "Permuting layer 30\n",
            "Permuting layer 31\n",
            "model.embed_tokens.weight                        -> token_embd.weight                        | BF16   | [32000, 4096]\n",
            "model.layers.0.input_layernorm.weight            -> blk.0.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.0.mlp.down_proj.weight              -> blk.0.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.0.mlp.gate_proj.weight              -> blk.0.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.0.mlp.up_proj.weight                -> blk.0.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.0.post_attention_layernorm.weight   -> blk.0.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.0.self_attn.k_proj.weight           -> blk.0.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.0.self_attn.o_proj.weight           -> blk.0.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.0.self_attn.q_proj.weight           -> blk.0.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.0.self_attn.v_proj.weight           -> blk.0.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.1.input_layernorm.weight            -> blk.1.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.1.mlp.down_proj.weight              -> blk.1.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.1.mlp.gate_proj.weight              -> blk.1.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.1.mlp.up_proj.weight                -> blk.1.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.1.post_attention_layernorm.weight   -> blk.1.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.1.self_attn.k_proj.weight           -> blk.1.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.1.self_attn.o_proj.weight           -> blk.1.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.1.self_attn.q_proj.weight           -> blk.1.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.1.self_attn.v_proj.weight           -> blk.1.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.10.mlp.gate_proj.weight             -> blk.10.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.10.mlp.up_proj.weight               -> blk.10.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.10.self_attn.k_proj.weight          -> blk.10.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.10.self_attn.o_proj.weight          -> blk.10.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.10.self_attn.q_proj.weight          -> blk.10.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.10.self_attn.v_proj.weight          -> blk.10.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.2.input_layernorm.weight            -> blk.2.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.2.mlp.down_proj.weight              -> blk.2.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.2.mlp.gate_proj.weight              -> blk.2.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.2.mlp.up_proj.weight                -> blk.2.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.2.post_attention_layernorm.weight   -> blk.2.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.2.self_attn.k_proj.weight           -> blk.2.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.2.self_attn.o_proj.weight           -> blk.2.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.2.self_attn.q_proj.weight           -> blk.2.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.2.self_attn.v_proj.weight           -> blk.2.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.3.input_layernorm.weight            -> blk.3.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.3.mlp.down_proj.weight              -> blk.3.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.3.mlp.gate_proj.weight              -> blk.3.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.3.mlp.up_proj.weight                -> blk.3.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.3.post_attention_layernorm.weight   -> blk.3.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.3.self_attn.k_proj.weight           -> blk.3.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.3.self_attn.o_proj.weight           -> blk.3.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.3.self_attn.q_proj.weight           -> blk.3.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.3.self_attn.v_proj.weight           -> blk.3.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.4.input_layernorm.weight            -> blk.4.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.4.mlp.down_proj.weight              -> blk.4.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.4.mlp.gate_proj.weight              -> blk.4.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.4.mlp.up_proj.weight                -> blk.4.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.4.post_attention_layernorm.weight   -> blk.4.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.4.self_attn.k_proj.weight           -> blk.4.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.4.self_attn.o_proj.weight           -> blk.4.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.4.self_attn.q_proj.weight           -> blk.4.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.4.self_attn.v_proj.weight           -> blk.4.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.5.input_layernorm.weight            -> blk.5.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.5.mlp.down_proj.weight              -> blk.5.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.5.mlp.gate_proj.weight              -> blk.5.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.5.mlp.up_proj.weight                -> blk.5.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.5.post_attention_layernorm.weight   -> blk.5.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.5.self_attn.k_proj.weight           -> blk.5.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.5.self_attn.o_proj.weight           -> blk.5.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.5.self_attn.q_proj.weight           -> blk.5.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.5.self_attn.v_proj.weight           -> blk.5.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.6.input_layernorm.weight            -> blk.6.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.6.mlp.down_proj.weight              -> blk.6.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.6.mlp.gate_proj.weight              -> blk.6.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.6.mlp.up_proj.weight                -> blk.6.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.6.post_attention_layernorm.weight   -> blk.6.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.6.self_attn.k_proj.weight           -> blk.6.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.6.self_attn.o_proj.weight           -> blk.6.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.6.self_attn.q_proj.weight           -> blk.6.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.6.self_attn.v_proj.weight           -> blk.6.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.7.input_layernorm.weight            -> blk.7.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.7.mlp.down_proj.weight              -> blk.7.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.7.mlp.gate_proj.weight              -> blk.7.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.7.mlp.up_proj.weight                -> blk.7.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.7.post_attention_layernorm.weight   -> blk.7.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.7.self_attn.k_proj.weight           -> blk.7.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.7.self_attn.o_proj.weight           -> blk.7.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.7.self_attn.q_proj.weight           -> blk.7.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.7.self_attn.v_proj.weight           -> blk.7.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.8.input_layernorm.weight            -> blk.8.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.8.mlp.down_proj.weight              -> blk.8.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.8.mlp.gate_proj.weight              -> blk.8.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.8.mlp.up_proj.weight                -> blk.8.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.8.post_attention_layernorm.weight   -> blk.8.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.8.self_attn.k_proj.weight           -> blk.8.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.8.self_attn.o_proj.weight           -> blk.8.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.8.self_attn.q_proj.weight           -> blk.8.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.8.self_attn.v_proj.weight           -> blk.8.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.9.input_layernorm.weight            -> blk.9.attn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.9.mlp.down_proj.weight              -> blk.9.ffn_down.weight                    | BF16   | [4096, 14336]\n",
            "model.layers.9.mlp.gate_proj.weight              -> blk.9.ffn_gate.weight                    | BF16   | [14336, 4096]\n",
            "model.layers.9.mlp.up_proj.weight                -> blk.9.ffn_up.weight                      | BF16   | [14336, 4096]\n",
            "model.layers.9.post_attention_layernorm.weight   -> blk.9.ffn_norm.weight                    | BF16   | [4096]\n",
            "model.layers.9.self_attn.k_proj.weight           -> blk.9.attn_k.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.9.self_attn.o_proj.weight           -> blk.9.attn_output.weight                 | BF16   | [4096, 4096]\n",
            "model.layers.9.self_attn.q_proj.weight           -> blk.9.attn_q.weight                      | BF16   | [4096, 4096]\n",
            "model.layers.9.self_attn.v_proj.weight           -> blk.9.attn_v.weight                      | BF16   | [1024, 4096]\n",
            "model.layers.10.input_layernorm.weight           -> blk.10.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.10.mlp.down_proj.weight             -> blk.10.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.10.post_attention_layernorm.weight  -> blk.10.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.11.input_layernorm.weight           -> blk.11.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.11.mlp.down_proj.weight             -> blk.11.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.11.mlp.gate_proj.weight             -> blk.11.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.11.mlp.up_proj.weight               -> blk.11.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.11.post_attention_layernorm.weight  -> blk.11.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.11.self_attn.k_proj.weight          -> blk.11.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.11.self_attn.o_proj.weight          -> blk.11.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.11.self_attn.q_proj.weight          -> blk.11.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.11.self_attn.v_proj.weight          -> blk.11.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.12.input_layernorm.weight           -> blk.12.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.12.mlp.down_proj.weight             -> blk.12.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.12.mlp.gate_proj.weight             -> blk.12.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.12.mlp.up_proj.weight               -> blk.12.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.12.post_attention_layernorm.weight  -> blk.12.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.12.self_attn.k_proj.weight          -> blk.12.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.12.self_attn.o_proj.weight          -> blk.12.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.12.self_attn.q_proj.weight          -> blk.12.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.12.self_attn.v_proj.weight          -> blk.12.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.13.input_layernorm.weight           -> blk.13.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.13.mlp.down_proj.weight             -> blk.13.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.13.mlp.gate_proj.weight             -> blk.13.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.13.mlp.up_proj.weight               -> blk.13.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.13.post_attention_layernorm.weight  -> blk.13.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.13.self_attn.k_proj.weight          -> blk.13.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.13.self_attn.o_proj.weight          -> blk.13.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.13.self_attn.q_proj.weight          -> blk.13.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.13.self_attn.v_proj.weight          -> blk.13.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.14.input_layernorm.weight           -> blk.14.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.14.mlp.down_proj.weight             -> blk.14.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.14.mlp.gate_proj.weight             -> blk.14.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.14.mlp.up_proj.weight               -> blk.14.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.14.post_attention_layernorm.weight  -> blk.14.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.14.self_attn.k_proj.weight          -> blk.14.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.14.self_attn.o_proj.weight          -> blk.14.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.14.self_attn.q_proj.weight          -> blk.14.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.14.self_attn.v_proj.weight          -> blk.14.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.15.input_layernorm.weight           -> blk.15.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.15.mlp.down_proj.weight             -> blk.15.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.15.mlp.gate_proj.weight             -> blk.15.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.15.mlp.up_proj.weight               -> blk.15.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.15.post_attention_layernorm.weight  -> blk.15.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.15.self_attn.k_proj.weight          -> blk.15.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.15.self_attn.o_proj.weight          -> blk.15.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.15.self_attn.q_proj.weight          -> blk.15.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.15.self_attn.v_proj.weight          -> blk.15.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.16.input_layernorm.weight           -> blk.16.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.16.mlp.down_proj.weight             -> blk.16.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.16.mlp.gate_proj.weight             -> blk.16.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.16.mlp.up_proj.weight               -> blk.16.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.16.post_attention_layernorm.weight  -> blk.16.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.16.self_attn.k_proj.weight          -> blk.16.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.16.self_attn.o_proj.weight          -> blk.16.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.16.self_attn.q_proj.weight          -> blk.16.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.16.self_attn.v_proj.weight          -> blk.16.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.17.input_layernorm.weight           -> blk.17.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.17.mlp.down_proj.weight             -> blk.17.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.17.mlp.gate_proj.weight             -> blk.17.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.17.mlp.up_proj.weight               -> blk.17.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.17.post_attention_layernorm.weight  -> blk.17.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.17.self_attn.k_proj.weight          -> blk.17.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.17.self_attn.o_proj.weight          -> blk.17.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.17.self_attn.q_proj.weight          -> blk.17.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.17.self_attn.v_proj.weight          -> blk.17.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.18.input_layernorm.weight           -> blk.18.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.18.mlp.down_proj.weight             -> blk.18.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.18.mlp.gate_proj.weight             -> blk.18.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.18.mlp.up_proj.weight               -> blk.18.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.18.post_attention_layernorm.weight  -> blk.18.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.18.self_attn.k_proj.weight          -> blk.18.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.18.self_attn.o_proj.weight          -> blk.18.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.18.self_attn.q_proj.weight          -> blk.18.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.18.self_attn.v_proj.weight          -> blk.18.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.19.input_layernorm.weight           -> blk.19.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.19.mlp.down_proj.weight             -> blk.19.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.19.mlp.gate_proj.weight             -> blk.19.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.19.mlp.up_proj.weight               -> blk.19.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.19.post_attention_layernorm.weight  -> blk.19.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.19.self_attn.k_proj.weight          -> blk.19.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.19.self_attn.o_proj.weight          -> blk.19.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.19.self_attn.q_proj.weight          -> blk.19.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.19.self_attn.v_proj.weight          -> blk.19.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.20.input_layernorm.weight           -> blk.20.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.20.mlp.down_proj.weight             -> blk.20.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.20.mlp.gate_proj.weight             -> blk.20.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.20.mlp.up_proj.weight               -> blk.20.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.20.post_attention_layernorm.weight  -> blk.20.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.20.self_attn.k_proj.weight          -> blk.20.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.20.self_attn.o_proj.weight          -> blk.20.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.20.self_attn.q_proj.weight          -> blk.20.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.20.self_attn.v_proj.weight          -> blk.20.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.21.input_layernorm.weight           -> blk.21.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.21.mlp.down_proj.weight             -> blk.21.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.21.mlp.gate_proj.weight             -> blk.21.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.21.mlp.up_proj.weight               -> blk.21.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.21.post_attention_layernorm.weight  -> blk.21.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.21.self_attn.k_proj.weight          -> blk.21.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.21.self_attn.o_proj.weight          -> blk.21.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.21.self_attn.q_proj.weight          -> blk.21.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.21.self_attn.v_proj.weight          -> blk.21.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.22.self_attn.k_proj.weight          -> blk.22.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.22.self_attn.o_proj.weight          -> blk.22.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.22.self_attn.q_proj.weight          -> blk.22.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.22.self_attn.v_proj.weight          -> blk.22.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "lm_head.weight                                   -> output.weight                            | BF16   | [32000, 4096]\n",
            "model.layers.22.input_layernorm.weight           -> blk.22.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.22.mlp.down_proj.weight             -> blk.22.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.22.mlp.gate_proj.weight             -> blk.22.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.22.mlp.up_proj.weight               -> blk.22.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.22.post_attention_layernorm.weight  -> blk.22.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.23.input_layernorm.weight           -> blk.23.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.23.mlp.down_proj.weight             -> blk.23.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.23.mlp.gate_proj.weight             -> blk.23.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.23.mlp.up_proj.weight               -> blk.23.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.23.post_attention_layernorm.weight  -> blk.23.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.23.self_attn.k_proj.weight          -> blk.23.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.23.self_attn.o_proj.weight          -> blk.23.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.23.self_attn.q_proj.weight          -> blk.23.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.23.self_attn.v_proj.weight          -> blk.23.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.24.input_layernorm.weight           -> blk.24.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.24.mlp.down_proj.weight             -> blk.24.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.24.mlp.gate_proj.weight             -> blk.24.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.24.mlp.up_proj.weight               -> blk.24.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.24.post_attention_layernorm.weight  -> blk.24.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.24.self_attn.k_proj.weight          -> blk.24.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.24.self_attn.o_proj.weight          -> blk.24.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.24.self_attn.q_proj.weight          -> blk.24.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.24.self_attn.v_proj.weight          -> blk.24.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.25.input_layernorm.weight           -> blk.25.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.25.mlp.down_proj.weight             -> blk.25.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.25.mlp.gate_proj.weight             -> blk.25.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.25.mlp.up_proj.weight               -> blk.25.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.25.post_attention_layernorm.weight  -> blk.25.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.25.self_attn.k_proj.weight          -> blk.25.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.25.self_attn.o_proj.weight          -> blk.25.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.25.self_attn.q_proj.weight          -> blk.25.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.25.self_attn.v_proj.weight          -> blk.25.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.26.input_layernorm.weight           -> blk.26.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.26.mlp.down_proj.weight             -> blk.26.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.26.mlp.gate_proj.weight             -> blk.26.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.26.mlp.up_proj.weight               -> blk.26.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.26.post_attention_layernorm.weight  -> blk.26.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.26.self_attn.k_proj.weight          -> blk.26.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.26.self_attn.o_proj.weight          -> blk.26.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.26.self_attn.q_proj.weight          -> blk.26.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.26.self_attn.v_proj.weight          -> blk.26.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.27.input_layernorm.weight           -> blk.27.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.27.mlp.down_proj.weight             -> blk.27.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.27.mlp.gate_proj.weight             -> blk.27.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.27.mlp.up_proj.weight               -> blk.27.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.27.post_attention_layernorm.weight  -> blk.27.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.27.self_attn.k_proj.weight          -> blk.27.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.27.self_attn.o_proj.weight          -> blk.27.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.27.self_attn.q_proj.weight          -> blk.27.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.27.self_attn.v_proj.weight          -> blk.27.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.28.input_layernorm.weight           -> blk.28.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.28.mlp.down_proj.weight             -> blk.28.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.28.mlp.gate_proj.weight             -> blk.28.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.28.mlp.up_proj.weight               -> blk.28.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.28.post_attention_layernorm.weight  -> blk.28.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.28.self_attn.k_proj.weight          -> blk.28.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.28.self_attn.o_proj.weight          -> blk.28.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.28.self_attn.q_proj.weight          -> blk.28.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.28.self_attn.v_proj.weight          -> blk.28.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.29.input_layernorm.weight           -> blk.29.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.29.mlp.down_proj.weight             -> blk.29.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.29.mlp.gate_proj.weight             -> blk.29.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.29.mlp.up_proj.weight               -> blk.29.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.29.post_attention_layernorm.weight  -> blk.29.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.29.self_attn.k_proj.weight          -> blk.29.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.29.self_attn.o_proj.weight          -> blk.29.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.29.self_attn.q_proj.weight          -> blk.29.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.29.self_attn.v_proj.weight          -> blk.29.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.30.input_layernorm.weight           -> blk.30.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.30.mlp.down_proj.weight             -> blk.30.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.30.mlp.gate_proj.weight             -> blk.30.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.30.mlp.up_proj.weight               -> blk.30.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.30.post_attention_layernorm.weight  -> blk.30.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.30.self_attn.k_proj.weight          -> blk.30.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.30.self_attn.o_proj.weight          -> blk.30.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.30.self_attn.q_proj.weight          -> blk.30.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.30.self_attn.v_proj.weight          -> blk.30.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.31.input_layernorm.weight           -> blk.31.attn_norm.weight                  | BF16   | [4096]\n",
            "model.layers.31.mlp.down_proj.weight             -> blk.31.ffn_down.weight                   | BF16   | [4096, 14336]\n",
            "model.layers.31.mlp.gate_proj.weight             -> blk.31.ffn_gate.weight                   | BF16   | [14336, 4096]\n",
            "model.layers.31.mlp.up_proj.weight               -> blk.31.ffn_up.weight                     | BF16   | [14336, 4096]\n",
            "model.layers.31.post_attention_layernorm.weight  -> blk.31.ffn_norm.weight                   | BF16   | [4096]\n",
            "model.layers.31.self_attn.k_proj.weight          -> blk.31.attn_k.weight                     | BF16   | [1024, 4096]\n",
            "model.layers.31.self_attn.o_proj.weight          -> blk.31.attn_output.weight                | BF16   | [4096, 4096]\n",
            "model.layers.31.self_attn.q_proj.weight          -> blk.31.attn_q.weight                     | BF16   | [4096, 4096]\n",
            "model.layers.31.self_attn.v_proj.weight          -> blk.31.attn_v.weight                     | BF16   | [1024, 4096]\n",
            "model.norm.weight                                -> output_norm.weight                       | BF16   | [4096]\n",
            "Writing Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.fp16.bin, format 1\n",
            "gguf: This GGUF file is for Little Endian only\n",
            "gguf: Adding 58980 merge(s).\n",
            "gguf: Setting special token type bos to 1\n",
            "gguf: Setting special token type eos to 2\n",
            "gguf: Setting special token type unk to 0\n",
            "gguf: Setting add_bos_token to True\n",
            "gguf: Setting add_eos_token to False\n",
            "gguf: Setting chat_template to {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n",
            "[  1/291] Writing tensor token_embd.weight                      | size  32000 x   4096  | type F16  | T+   1\n",
            "[  2/291] Writing tensor blk.0.attn_norm.weight                 | size   4096           | type F32  | T+   1\n",
            "[  3/291] Writing tensor blk.0.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   1\n",
            "[  4/291] Writing tensor blk.0.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   1\n",
            "[  5/291] Writing tensor blk.0.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   1\n",
            "[  6/291] Writing tensor blk.0.ffn_norm.weight                  | size   4096           | type F32  | T+   2\n",
            "[  7/291] Writing tensor blk.0.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
            "[  8/291] Writing tensor blk.0.attn_output.weight               | size   4096 x   4096  | type F16  | T+   2\n",
            "[  9/291] Writing tensor blk.0.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   2\n",
            "[ 10/291] Writing tensor blk.0.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   2\n",
            "[ 11/291] Writing tensor blk.1.attn_norm.weight                 | size   4096           | type F32  | T+   2\n",
            "[ 12/291] Writing tensor blk.1.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   2\n",
            "[ 13/291] Writing tensor blk.1.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   2\n",
            "[ 14/291] Writing tensor blk.1.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   2\n",
            "[ 15/291] Writing tensor blk.1.ffn_norm.weight                  | size   4096           | type F32  | T+   3\n",
            "[ 16/291] Writing tensor blk.1.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
            "[ 17/291] Writing tensor blk.1.attn_output.weight               | size   4096 x   4096  | type F16  | T+   3\n",
            "[ 18/291] Writing tensor blk.1.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   3\n",
            "[ 19/291] Writing tensor blk.1.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   3\n",
            "[ 20/291] Writing tensor blk.10.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+   3\n",
            "[ 21/291] Writing tensor blk.10.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+   3\n",
            "[ 22/291] Writing tensor blk.10.attn_k.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
            "[ 23/291] Writing tensor blk.10.attn_output.weight              | size   4096 x   4096  | type F16  | T+   3\n",
            "[ 24/291] Writing tensor blk.10.attn_q.weight                   | size   4096 x   4096  | type F16  | T+   3\n",
            "[ 25/291] Writing tensor blk.10.attn_v.weight                   | size   1024 x   4096  | type F16  | T+   3\n",
            "[ 26/291] Writing tensor blk.2.attn_norm.weight                 | size   4096           | type F32  | T+   3\n",
            "[ 27/291] Writing tensor blk.2.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   3\n",
            "[ 28/291] Writing tensor blk.2.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   3\n",
            "[ 29/291] Writing tensor blk.2.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   4\n",
            "[ 30/291] Writing tensor blk.2.ffn_norm.weight                  | size   4096           | type F32  | T+   4\n",
            "[ 31/291] Writing tensor blk.2.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
            "[ 32/291] Writing tensor blk.2.attn_output.weight               | size   4096 x   4096  | type F16  | T+   4\n",
            "[ 33/291] Writing tensor blk.2.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   4\n",
            "[ 34/291] Writing tensor blk.2.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   4\n",
            "[ 35/291] Writing tensor blk.3.attn_norm.weight                 | size   4096           | type F32  | T+   4\n",
            "[ 36/291] Writing tensor blk.3.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   4\n",
            "[ 37/291] Writing tensor blk.3.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   5\n",
            "[ 38/291] Writing tensor blk.3.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   5\n",
            "[ 39/291] Writing tensor blk.3.ffn_norm.weight                  | size   4096           | type F32  | T+   5\n",
            "[ 40/291] Writing tensor blk.3.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   5\n",
            "[ 41/291] Writing tensor blk.3.attn_output.weight               | size   4096 x   4096  | type F16  | T+   5\n",
            "[ 42/291] Writing tensor blk.3.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   5\n",
            "[ 43/291] Writing tensor blk.3.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   6\n",
            "[ 44/291] Writing tensor blk.4.attn_norm.weight                 | size   4096           | type F32  | T+   6\n",
            "[ 45/291] Writing tensor blk.4.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   6\n",
            "[ 46/291] Writing tensor blk.4.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   6\n",
            "[ 47/291] Writing tensor blk.4.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   6\n",
            "[ 48/291] Writing tensor blk.4.ffn_norm.weight                  | size   4096           | type F32  | T+   7\n",
            "[ 49/291] Writing tensor blk.4.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   7\n",
            "[ 50/291] Writing tensor blk.4.attn_output.weight               | size   4096 x   4096  | type F16  | T+   7\n",
            "[ 51/291] Writing tensor blk.4.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   7\n",
            "[ 52/291] Writing tensor blk.4.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   7\n",
            "[ 53/291] Writing tensor blk.5.attn_norm.weight                 | size   4096           | type F32  | T+   7\n",
            "[ 54/291] Writing tensor blk.5.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   7\n",
            "[ 55/291] Writing tensor blk.5.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   7\n",
            "[ 56/291] Writing tensor blk.5.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   8\n",
            "[ 57/291] Writing tensor blk.5.ffn_norm.weight                  | size   4096           | type F32  | T+   8\n",
            "[ 58/291] Writing tensor blk.5.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   8\n",
            "[ 59/291] Writing tensor blk.5.attn_output.weight               | size   4096 x   4096  | type F16  | T+   8\n",
            "[ 60/291] Writing tensor blk.5.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   8\n",
            "[ 61/291] Writing tensor blk.5.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   8\n",
            "[ 62/291] Writing tensor blk.6.attn_norm.weight                 | size   4096           | type F32  | T+   8\n",
            "[ 63/291] Writing tensor blk.6.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   8\n",
            "[ 64/291] Writing tensor blk.6.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+   8\n",
            "[ 65/291] Writing tensor blk.6.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+   9\n",
            "[ 66/291] Writing tensor blk.6.ffn_norm.weight                  | size   4096           | type F32  | T+   9\n",
            "[ 67/291] Writing tensor blk.6.attn_k.weight                    | size   1024 x   4096  | type F16  | T+   9\n",
            "[ 68/291] Writing tensor blk.6.attn_output.weight               | size   4096 x   4096  | type F16  | T+   9\n",
            "[ 69/291] Writing tensor blk.6.attn_q.weight                    | size   4096 x   4096  | type F16  | T+   9\n",
            "[ 70/291] Writing tensor blk.6.attn_v.weight                    | size   1024 x   4096  | type F16  | T+   9\n",
            "[ 71/291] Writing tensor blk.7.attn_norm.weight                 | size   4096           | type F32  | T+   9\n",
            "[ 72/291] Writing tensor blk.7.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+   9\n",
            "[ 73/291] Writing tensor blk.7.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  10\n",
            "[ 74/291] Writing tensor blk.7.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  10\n",
            "[ 75/291] Writing tensor blk.7.ffn_norm.weight                  | size   4096           | type F32  | T+  10\n",
            "[ 76/291] Writing tensor blk.7.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  10\n",
            "[ 77/291] Writing tensor blk.7.attn_output.weight               | size   4096 x   4096  | type F16  | T+  10\n",
            "[ 78/291] Writing tensor blk.7.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  11\n",
            "[ 79/291] Writing tensor blk.7.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  11\n",
            "[ 80/291] Writing tensor blk.8.attn_norm.weight                 | size   4096           | type F32  | T+  11\n",
            "[ 81/291] Writing tensor blk.8.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  11\n",
            "[ 82/291] Writing tensor blk.8.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  11\n",
            "[ 83/291] Writing tensor blk.8.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  11\n",
            "[ 84/291] Writing tensor blk.8.ffn_norm.weight                  | size   4096           | type F32  | T+  12\n",
            "[ 85/291] Writing tensor blk.8.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  12\n",
            "[ 86/291] Writing tensor blk.8.attn_output.weight               | size   4096 x   4096  | type F16  | T+  12\n",
            "[ 87/291] Writing tensor blk.8.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  12\n",
            "[ 88/291] Writing tensor blk.8.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  12\n",
            "[ 89/291] Writing tensor blk.9.attn_norm.weight                 | size   4096           | type F32  | T+  12\n",
            "[ 90/291] Writing tensor blk.9.ffn_down.weight                  | size   4096 x  14336  | type F16  | T+  12\n",
            "[ 91/291] Writing tensor blk.9.ffn_gate.weight                  | size  14336 x   4096  | type F16  | T+  12\n",
            "[ 92/291] Writing tensor blk.9.ffn_up.weight                    | size  14336 x   4096  | type F16  | T+  13\n",
            "[ 93/291] Writing tensor blk.9.ffn_norm.weight                  | size   4096           | type F32  | T+  13\n",
            "[ 94/291] Writing tensor blk.9.attn_k.weight                    | size   1024 x   4096  | type F16  | T+  13\n",
            "[ 95/291] Writing tensor blk.9.attn_output.weight               | size   4096 x   4096  | type F16  | T+  13\n",
            "[ 96/291] Writing tensor blk.9.attn_q.weight                    | size   4096 x   4096  | type F16  | T+  13\n",
            "[ 97/291] Writing tensor blk.9.attn_v.weight                    | size   1024 x   4096  | type F16  | T+  13\n",
            "[ 98/291] Writing tensor blk.10.attn_norm.weight                | size   4096           | type F32  | T+  13\n",
            "[ 99/291] Writing tensor blk.10.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  13\n",
            "[100/291] Writing tensor blk.10.ffn_norm.weight                 | size   4096           | type F32  | T+  14\n",
            "[101/291] Writing tensor blk.11.attn_norm.weight                | size   4096           | type F32  | T+  14\n",
            "[102/291] Writing tensor blk.11.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  14\n",
            "[103/291] Writing tensor blk.11.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  14\n",
            "[104/291] Writing tensor blk.11.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  14\n",
            "[105/291] Writing tensor blk.11.ffn_norm.weight                 | size   4096           | type F32  | T+  15\n",
            "[106/291] Writing tensor blk.11.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  15\n",
            "[107/291] Writing tensor blk.11.attn_output.weight              | size   4096 x   4096  | type F16  | T+  15\n",
            "[108/291] Writing tensor blk.11.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  15\n",
            "[109/291] Writing tensor blk.11.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  15\n",
            "[110/291] Writing tensor blk.12.attn_norm.weight                | size   4096           | type F32  | T+  15\n",
            "[111/291] Writing tensor blk.12.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  15\n",
            "[112/291] Writing tensor blk.12.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  15\n",
            "[113/291] Writing tensor blk.12.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  16\n",
            "[114/291] Writing tensor blk.12.ffn_norm.weight                 | size   4096           | type F32  | T+  16\n",
            "[115/291] Writing tensor blk.12.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  16\n",
            "[116/291] Writing tensor blk.12.attn_output.weight              | size   4096 x   4096  | type F16  | T+  16\n",
            "[117/291] Writing tensor blk.12.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  16\n",
            "[118/291] Writing tensor blk.12.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  16\n",
            "[119/291] Writing tensor blk.13.attn_norm.weight                | size   4096           | type F32  | T+  16\n",
            "[120/291] Writing tensor blk.13.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  16\n",
            "[121/291] Writing tensor blk.13.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  16\n",
            "[122/291] Writing tensor blk.13.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  17\n",
            "[123/291] Writing tensor blk.13.ffn_norm.weight                 | size   4096           | type F32  | T+  17\n",
            "[124/291] Writing tensor blk.13.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  17\n",
            "[125/291] Writing tensor blk.13.attn_output.weight              | size   4096 x   4096  | type F16  | T+  17\n",
            "[126/291] Writing tensor blk.13.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  17\n",
            "[127/291] Writing tensor blk.13.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  17\n",
            "[128/291] Writing tensor blk.14.attn_norm.weight                | size   4096           | type F32  | T+  17\n",
            "[129/291] Writing tensor blk.14.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  17\n",
            "[130/291] Writing tensor blk.14.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  18\n",
            "[131/291] Writing tensor blk.14.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  18\n",
            "[132/291] Writing tensor blk.14.ffn_norm.weight                 | size   4096           | type F32  | T+  18\n",
            "[133/291] Writing tensor blk.14.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  18\n",
            "[134/291] Writing tensor blk.14.attn_output.weight              | size   4096 x   4096  | type F16  | T+  18\n",
            "[135/291] Writing tensor blk.14.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  19\n",
            "[136/291] Writing tensor blk.14.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  19\n",
            "[137/291] Writing tensor blk.15.attn_norm.weight                | size   4096           | type F32  | T+  19\n",
            "[138/291] Writing tensor blk.15.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  19\n",
            "[139/291] Writing tensor blk.15.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  19\n",
            "[140/291] Writing tensor blk.15.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  19\n",
            "[141/291] Writing tensor blk.15.ffn_norm.weight                 | size   4096           | type F32  | T+  20\n",
            "[142/291] Writing tensor blk.15.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  20\n",
            "[143/291] Writing tensor blk.15.attn_output.weight              | size   4096 x   4096  | type F16  | T+  20\n",
            "[144/291] Writing tensor blk.15.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  20\n",
            "[145/291] Writing tensor blk.15.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  20\n",
            "[146/291] Writing tensor blk.16.attn_norm.weight                | size   4096           | type F32  | T+  20\n",
            "[147/291] Writing tensor blk.16.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  20\n",
            "[148/291] Writing tensor blk.16.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  20\n",
            "[149/291] Writing tensor blk.16.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  21\n",
            "[150/291] Writing tensor blk.16.ffn_norm.weight                 | size   4096           | type F32  | T+  21\n",
            "[151/291] Writing tensor blk.16.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  21\n",
            "[152/291] Writing tensor blk.16.attn_output.weight              | size   4096 x   4096  | type F16  | T+  21\n",
            "[153/291] Writing tensor blk.16.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  21\n",
            "[154/291] Writing tensor blk.16.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  21\n",
            "[155/291] Writing tensor blk.17.attn_norm.weight                | size   4096           | type F32  | T+  21\n",
            "[156/291] Writing tensor blk.17.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  21\n",
            "[157/291] Writing tensor blk.17.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  22\n",
            "[158/291] Writing tensor blk.17.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  22\n",
            "[159/291] Writing tensor blk.17.ffn_norm.weight                 | size   4096           | type F32  | T+  22\n",
            "[160/291] Writing tensor blk.17.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  22\n",
            "[161/291] Writing tensor blk.17.attn_output.weight              | size   4096 x   4096  | type F16  | T+  22\n",
            "[162/291] Writing tensor blk.17.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  22\n",
            "[163/291] Writing tensor blk.17.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  22\n",
            "[164/291] Writing tensor blk.18.attn_norm.weight                | size   4096           | type F32  | T+  22\n",
            "[165/291] Writing tensor blk.18.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  22\n",
            "[166/291] Writing tensor blk.18.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  23\n",
            "[167/291] Writing tensor blk.18.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  23\n",
            "[168/291] Writing tensor blk.18.ffn_norm.weight                 | size   4096           | type F32  | T+  24\n",
            "[169/291] Writing tensor blk.18.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  24\n",
            "[170/291] Writing tensor blk.18.attn_output.weight              | size   4096 x   4096  | type F16  | T+  24\n",
            "[171/291] Writing tensor blk.18.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  24\n",
            "[172/291] Writing tensor blk.18.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  24\n",
            "[173/291] Writing tensor blk.19.attn_norm.weight                | size   4096           | type F32  | T+  24\n",
            "[174/291] Writing tensor blk.19.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  24\n",
            "[175/291] Writing tensor blk.19.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  24\n",
            "[176/291] Writing tensor blk.19.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  24\n",
            "[177/291] Writing tensor blk.19.ffn_norm.weight                 | size   4096           | type F32  | T+  25\n",
            "[178/291] Writing tensor blk.19.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  25\n",
            "[179/291] Writing tensor blk.19.attn_output.weight              | size   4096 x   4096  | type F16  | T+  25\n",
            "[180/291] Writing tensor blk.19.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  25\n",
            "[181/291] Writing tensor blk.19.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  25\n",
            "[182/291] Writing tensor blk.20.attn_norm.weight                | size   4096           | type F32  | T+  25\n",
            "[183/291] Writing tensor blk.20.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  25\n",
            "[184/291] Writing tensor blk.20.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  25\n",
            "[185/291] Writing tensor blk.20.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  26\n",
            "[186/291] Writing tensor blk.20.ffn_norm.weight                 | size   4096           | type F32  | T+  26\n",
            "[187/291] Writing tensor blk.20.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  26\n",
            "[188/291] Writing tensor blk.20.attn_output.weight              | size   4096 x   4096  | type F16  | T+  26\n",
            "[189/291] Writing tensor blk.20.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  26\n",
            "[190/291] Writing tensor blk.20.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  27\n",
            "[191/291] Writing tensor blk.21.attn_norm.weight                | size   4096           | type F32  | T+  27\n",
            "[192/291] Writing tensor blk.21.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  27\n",
            "[193/291] Writing tensor blk.21.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  27\n",
            "[194/291] Writing tensor blk.21.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  27\n",
            "[195/291] Writing tensor blk.21.ffn_norm.weight                 | size   4096           | type F32  | T+  28\n",
            "[196/291] Writing tensor blk.21.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  28\n",
            "[197/291] Writing tensor blk.21.attn_output.weight              | size   4096 x   4096  | type F16  | T+  28\n",
            "[198/291] Writing tensor blk.21.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  28\n",
            "[199/291] Writing tensor blk.21.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  28\n",
            "[200/291] Writing tensor blk.22.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  28\n",
            "[201/291] Writing tensor blk.22.attn_output.weight              | size   4096 x   4096  | type F16  | T+  28\n",
            "[202/291] Writing tensor blk.22.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  28\n",
            "[203/291] Writing tensor blk.22.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  29\n",
            "[204/291] Writing tensor output.weight                          | size  32000 x   4096  | type F16  | T+  32\n",
            "[205/291] Writing tensor blk.22.attn_norm.weight                | size   4096           | type F32  | T+  33\n",
            "[206/291] Writing tensor blk.22.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  33\n",
            "[207/291] Writing tensor blk.22.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  33\n",
            "[208/291] Writing tensor blk.22.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  33\n",
            "[209/291] Writing tensor blk.22.ffn_norm.weight                 | size   4096           | type F32  | T+  33\n",
            "[210/291] Writing tensor blk.23.attn_norm.weight                | size   4096           | type F32  | T+  33\n",
            "[211/291] Writing tensor blk.23.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  33\n",
            "[212/291] Writing tensor blk.23.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  34\n",
            "[213/291] Writing tensor blk.23.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  35\n",
            "[214/291] Writing tensor blk.23.ffn_norm.weight                 | size   4096           | type F32  | T+  35\n",
            "[215/291] Writing tensor blk.23.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  35\n",
            "[216/291] Writing tensor blk.23.attn_output.weight              | size   4096 x   4096  | type F16  | T+  35\n",
            "[217/291] Writing tensor blk.23.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  35\n",
            "[218/291] Writing tensor blk.23.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  35\n",
            "[219/291] Writing tensor blk.24.attn_norm.weight                | size   4096           | type F32  | T+  35\n",
            "[220/291] Writing tensor blk.24.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  36\n",
            "[221/291] Writing tensor blk.24.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  37\n",
            "[222/291] Writing tensor blk.24.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  37\n",
            "[223/291] Writing tensor blk.24.ffn_norm.weight                 | size   4096           | type F32  | T+  37\n",
            "[224/291] Writing tensor blk.24.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  37\n",
            "[225/291] Writing tensor blk.24.attn_output.weight              | size   4096 x   4096  | type F16  | T+  37\n",
            "[226/291] Writing tensor blk.24.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  37\n",
            "[227/291] Writing tensor blk.24.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  37\n",
            "[228/291] Writing tensor blk.25.attn_norm.weight                | size   4096           | type F32  | T+  37\n",
            "[229/291] Writing tensor blk.25.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  39\n",
            "[230/291] Writing tensor blk.25.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  39\n",
            "[231/291] Writing tensor blk.25.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  39\n",
            "[232/291] Writing tensor blk.25.ffn_norm.weight                 | size   4096           | type F32  | T+  39\n",
            "[233/291] Writing tensor blk.25.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  39\n",
            "[234/291] Writing tensor blk.25.attn_output.weight              | size   4096 x   4096  | type F16  | T+  39\n",
            "[235/291] Writing tensor blk.25.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  39\n",
            "[236/291] Writing tensor blk.25.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  39\n",
            "[237/291] Writing tensor blk.26.attn_norm.weight                | size   4096           | type F32  | T+  39\n",
            "[238/291] Writing tensor blk.26.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  41\n",
            "[239/291] Writing tensor blk.26.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  41\n",
            "[240/291] Writing tensor blk.26.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  41\n",
            "[241/291] Writing tensor blk.26.ffn_norm.weight                 | size   4096           | type F32  | T+  42\n",
            "[242/291] Writing tensor blk.26.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  42\n",
            "[243/291] Writing tensor blk.26.attn_output.weight              | size   4096 x   4096  | type F16  | T+  42\n",
            "[244/291] Writing tensor blk.26.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  42\n",
            "[245/291] Writing tensor blk.26.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  42\n",
            "[246/291] Writing tensor blk.27.attn_norm.weight                | size   4096           | type F32  | T+  42\n",
            "[247/291] Writing tensor blk.27.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  44\n",
            "[248/291] Writing tensor blk.27.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  44\n",
            "[249/291] Writing tensor blk.27.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  44\n",
            "[250/291] Writing tensor blk.27.ffn_norm.weight                 | size   4096           | type F32  | T+  44\n",
            "[251/291] Writing tensor blk.27.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  44\n",
            "[252/291] Writing tensor blk.27.attn_output.weight              | size   4096 x   4096  | type F16  | T+  44\n",
            "[253/291] Writing tensor blk.27.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  44\n",
            "[254/291] Writing tensor blk.27.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  44\n",
            "[255/291] Writing tensor blk.28.attn_norm.weight                | size   4096           | type F32  | T+  44\n",
            "[256/291] Writing tensor blk.28.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  46\n",
            "[257/291] Writing tensor blk.28.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  46\n",
            "[258/291] Writing tensor blk.28.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  46\n",
            "[259/291] Writing tensor blk.28.ffn_norm.weight                 | size   4096           | type F32  | T+  46\n",
            "[260/291] Writing tensor blk.28.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  46\n",
            "[261/291] Writing tensor blk.28.attn_output.weight              | size   4096 x   4096  | type F16  | T+  46\n",
            "[262/291] Writing tensor blk.28.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  46\n",
            "[263/291] Writing tensor blk.28.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  47\n",
            "[264/291] Writing tensor blk.29.attn_norm.weight                | size   4096           | type F32  | T+  47\n",
            "[265/291] Writing tensor blk.29.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  49\n",
            "[266/291] Writing tensor blk.29.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  49\n",
            "[267/291] Writing tensor blk.29.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  49\n",
            "[268/291] Writing tensor blk.29.ffn_norm.weight                 | size   4096           | type F32  | T+  49\n",
            "[269/291] Writing tensor blk.29.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  49\n",
            "[270/291] Writing tensor blk.29.attn_output.weight              | size   4096 x   4096  | type F16  | T+  49\n",
            "[271/291] Writing tensor blk.29.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  50\n",
            "[272/291] Writing tensor blk.29.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  50\n",
            "[273/291] Writing tensor blk.30.attn_norm.weight                | size   4096           | type F32  | T+  50\n",
            "[274/291] Writing tensor blk.30.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  52\n",
            "[275/291] Writing tensor blk.30.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  52\n",
            "[276/291] Writing tensor blk.30.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  52\n",
            "[277/291] Writing tensor blk.30.ffn_norm.weight                 | size   4096           | type F32  | T+  52\n",
            "[278/291] Writing tensor blk.30.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  52\n",
            "[279/291] Writing tensor blk.30.attn_output.weight              | size   4096 x   4096  | type F16  | T+  52\n",
            "[280/291] Writing tensor blk.30.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  52\n",
            "[281/291] Writing tensor blk.30.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  52\n",
            "[282/291] Writing tensor blk.31.attn_norm.weight                | size   4096           | type F32  | T+  52\n",
            "[283/291] Writing tensor blk.31.ffn_down.weight                 | size   4096 x  14336  | type F16  | T+  55\n",
            "[284/291] Writing tensor blk.31.ffn_gate.weight                 | size  14336 x   4096  | type F16  | T+  55\n",
            "[285/291] Writing tensor blk.31.ffn_up.weight                   | size  14336 x   4096  | type F16  | T+  55\n",
            "[286/291] Writing tensor blk.31.ffn_norm.weight                 | size   4096           | type F32  | T+  55\n",
            "[287/291] Writing tensor blk.31.attn_k.weight                   | size   1024 x   4096  | type F16  | T+  55\n",
            "[288/291] Writing tensor blk.31.attn_output.weight              | size   4096 x   4096  | type F16  | T+  55\n",
            "[289/291] Writing tensor blk.31.attn_q.weight                   | size   4096 x   4096  | type F16  | T+  55\n",
            "[290/291] Writing tensor blk.31.attn_v.weight                   | size   1024 x   4096  | type F16  | T+  55\n",
            "[291/291] Writing tensor output_norm.weight                     | size   4096           | type F32  | T+  55\n",
            "Wrote Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.fp16.bin\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: build = 1803 (36e5a08)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.fp16.bin' to 'Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.Q5_K_M.gguf' as Q5_K_M\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = [\"▁ t\", \"i n\", \"e r\", \"▁ a\", \"h e...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "llama_model_quantize_internal: meta size = 1671744 bytes\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q5_K .. size =   250.00 MiB ->    85.94 MiB | hist: \n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  20/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  21/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  22/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  23/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  24/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  25/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  26/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  29/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  31/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  32/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  33/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  34/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  35/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  38/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  40/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  41/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  42/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  43/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  44/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  47/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  49/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  50/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  51/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  52/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  53/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  56/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  58/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  59/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  60/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  61/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  62/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  65/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  67/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  68/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  69/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  70/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  71/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  74/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  76/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  77/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  78/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  79/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  80/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  83/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  85/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  86/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  87/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  88/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  89/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  92/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  94/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  95/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  96/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[  97/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[  98/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 101/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 103/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 104/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 105/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 107/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 108/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 109/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 200/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 201/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 203/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 204/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \n",
            "[ 205/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 207/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 211/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 212/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 213/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 214/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 215/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 216/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 217/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 218/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 220/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 229/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 238/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 247/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 256/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 265/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 274/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 283/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q5_K .. size =   112.00 MiB ->    38.50 MiB | hist: \n",
            "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q5_K .. size =     8.00 MiB ->     2.75 MiB | hist: \n",
            "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q5_K .. size =    32.00 MiB ->    11.00 MiB | hist: \n",
            "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 13813.02 MB\n",
            "llama_model_quantize_internal: quant size  =  4892.99 MB\n",
            "\n",
            "main: quantize time = 244452.94 ms\n",
            "main:    total time = 244452.94 ms\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "main: build = 1803 (36e5a08)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: quantizing 'Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.fp16.bin' to 'Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.Q4_K_M.gguf' as Q4_K_M\n",
            "llama_model_loader: loaded meta data with 23 key-value pairs and 291 tensors from Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.fp16.bin (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 1\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = [\"▁ t\", \"i n\", \"e r\", \"▁ a\", \"h e...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type  f16:  226 tensors\n",
            "llama_model_quantize_internal: meta size = 1671744 bytes\n",
            "[   1/ 291]                    token_embd.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q4_K .. size =   250.00 MiB ->    70.31 MiB | hist: \n",
            "[   2/ 291]               blk.0.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   3/ 291]                blk.0.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[   4/ 291]                blk.0.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[   5/ 291]                  blk.0.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[   6/ 291]                blk.0.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[   7/ 291]                  blk.0.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[   8/ 291]             blk.0.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[   9/ 291]                  blk.0.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  10/ 291]                  blk.0.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  11/ 291]               blk.1.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  12/ 291]                blk.1.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  13/ 291]                blk.1.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  14/ 291]                  blk.1.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  15/ 291]                blk.1.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  16/ 291]                  blk.1.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  17/ 291]             blk.1.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  18/ 291]                  blk.1.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  19/ 291]                  blk.1.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  20/ 291]               blk.10.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  21/ 291]                 blk.10.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  22/ 291]                 blk.10.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  23/ 291]            blk.10.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  24/ 291]                 blk.10.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  25/ 291]                 blk.10.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  26/ 291]               blk.2.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  27/ 291]                blk.2.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  28/ 291]                blk.2.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  29/ 291]                  blk.2.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  30/ 291]                blk.2.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  31/ 291]                  blk.2.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  32/ 291]             blk.2.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  33/ 291]                  blk.2.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  34/ 291]                  blk.2.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  35/ 291]               blk.3.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  36/ 291]                blk.3.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  37/ 291]                blk.3.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  38/ 291]                  blk.3.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  39/ 291]                blk.3.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  40/ 291]                  blk.3.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  41/ 291]             blk.3.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  42/ 291]                  blk.3.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  43/ 291]                  blk.3.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  44/ 291]               blk.4.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  45/ 291]                blk.4.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  46/ 291]                blk.4.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  47/ 291]                  blk.4.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  48/ 291]                blk.4.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  49/ 291]                  blk.4.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  50/ 291]             blk.4.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  51/ 291]                  blk.4.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  52/ 291]                  blk.4.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  53/ 291]               blk.5.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  54/ 291]                blk.5.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  55/ 291]                blk.5.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  56/ 291]                  blk.5.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  57/ 291]                blk.5.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  58/ 291]                  blk.5.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  59/ 291]             blk.5.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  60/ 291]                  blk.5.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  61/ 291]                  blk.5.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  62/ 291]               blk.6.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  63/ 291]                blk.6.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  64/ 291]                blk.6.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  65/ 291]                  blk.6.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  66/ 291]                blk.6.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  67/ 291]                  blk.6.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  68/ 291]             blk.6.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  69/ 291]                  blk.6.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  70/ 291]                  blk.6.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  71/ 291]               blk.7.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  72/ 291]                blk.7.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  73/ 291]                blk.7.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  74/ 291]                  blk.7.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  75/ 291]                blk.7.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  76/ 291]                  blk.7.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  77/ 291]             blk.7.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  78/ 291]                  blk.7.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  79/ 291]                  blk.7.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  80/ 291]               blk.8.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  81/ 291]                blk.8.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  82/ 291]                blk.8.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  83/ 291]                  blk.8.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  84/ 291]                blk.8.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  85/ 291]                  blk.8.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  86/ 291]             blk.8.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  87/ 291]                  blk.8.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  88/ 291]                  blk.8.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[  89/ 291]               blk.9.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  90/ 291]                blk.9.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[  91/ 291]                blk.9.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  92/ 291]                  blk.9.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[  93/ 291]                blk.9.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  94/ 291]                  blk.9.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  95/ 291]             blk.9.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  96/ 291]                  blk.9.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[  97/ 291]                  blk.9.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[  98/ 291]              blk.10.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[  99/ 291]               blk.10.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 100/ 291]               blk.10.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 101/ 291]              blk.11.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 102/ 291]               blk.11.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 103/ 291]               blk.11.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 104/ 291]                 blk.11.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 105/ 291]               blk.11.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 106/ 291]                 blk.11.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 107/ 291]            blk.11.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 108/ 291]                 blk.11.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 109/ 291]                 blk.11.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 110/ 291]              blk.12.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 111/ 291]               blk.12.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 112/ 291]               blk.12.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 113/ 291]                 blk.12.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 114/ 291]               blk.12.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 115/ 291]                 blk.12.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 116/ 291]            blk.12.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 117/ 291]                 blk.12.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 118/ 291]                 blk.12.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 119/ 291]              blk.13.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 120/ 291]               blk.13.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 121/ 291]               blk.13.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 122/ 291]                 blk.13.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 123/ 291]               blk.13.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 124/ 291]                 blk.13.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 125/ 291]            blk.13.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 126/ 291]                 blk.13.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 127/ 291]                 blk.13.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 128/ 291]              blk.14.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 129/ 291]               blk.14.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 130/ 291]               blk.14.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 131/ 291]                 blk.14.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 132/ 291]               blk.14.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 133/ 291]                 blk.14.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 134/ 291]            blk.14.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 135/ 291]                 blk.14.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 136/ 291]                 blk.14.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 137/ 291]              blk.15.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 138/ 291]               blk.15.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 139/ 291]               blk.15.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 140/ 291]                 blk.15.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 141/ 291]               blk.15.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 142/ 291]                 blk.15.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 143/ 291]            blk.15.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 144/ 291]                 blk.15.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 145/ 291]                 blk.15.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 146/ 291]              blk.16.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 147/ 291]               blk.16.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 148/ 291]               blk.16.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 149/ 291]                 blk.16.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 150/ 291]               blk.16.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 151/ 291]                 blk.16.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 152/ 291]            blk.16.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 153/ 291]                 blk.16.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 154/ 291]                 blk.16.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 155/ 291]              blk.17.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 156/ 291]               blk.17.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 157/ 291]               blk.17.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 158/ 291]                 blk.17.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 159/ 291]               blk.17.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 160/ 291]                 blk.17.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 161/ 291]            blk.17.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 162/ 291]                 blk.17.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 163/ 291]                 blk.17.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 164/ 291]              blk.18.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 165/ 291]               blk.18.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 166/ 291]               blk.18.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 167/ 291]                 blk.18.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 168/ 291]               blk.18.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 169/ 291]                 blk.18.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 170/ 291]            blk.18.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 171/ 291]                 blk.18.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 172/ 291]                 blk.18.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 173/ 291]              blk.19.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 174/ 291]               blk.19.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 175/ 291]               blk.19.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 176/ 291]                 blk.19.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 177/ 291]               blk.19.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 178/ 291]                 blk.19.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 179/ 291]            blk.19.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 180/ 291]                 blk.19.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 181/ 291]                 blk.19.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 182/ 291]              blk.20.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 183/ 291]               blk.20.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 184/ 291]               blk.20.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 185/ 291]                 blk.20.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 186/ 291]               blk.20.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 187/ 291]                 blk.20.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 188/ 291]            blk.20.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 189/ 291]                 blk.20.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 190/ 291]                 blk.20.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 191/ 291]              blk.21.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 192/ 291]               blk.21.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 193/ 291]               blk.21.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 194/ 291]                 blk.21.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 195/ 291]               blk.21.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 196/ 291]                 blk.21.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 197/ 291]            blk.21.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 198/ 291]                 blk.21.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 199/ 291]                 blk.21.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 200/ 291]                 blk.22.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 201/ 291]            blk.22.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 202/ 291]                 blk.22.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 203/ 291]                 blk.22.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 204/ 291]                        output.weight - [ 4096, 32000,     1,     1], type =    f16, quantizing to q6_K .. size =   250.00 MiB ->   102.54 MiB | hist: \n",
            "[ 205/ 291]              blk.22.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 206/ 291]               blk.22.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 207/ 291]               blk.22.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 208/ 291]                 blk.22.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 209/ 291]               blk.22.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 210/ 291]              blk.23.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 211/ 291]               blk.23.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 212/ 291]               blk.23.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 213/ 291]                 blk.23.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 214/ 291]               blk.23.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 215/ 291]                 blk.23.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 216/ 291]            blk.23.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 217/ 291]                 blk.23.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 218/ 291]                 blk.23.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 219/ 291]              blk.24.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 220/ 291]               blk.24.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 221/ 291]               blk.24.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 222/ 291]                 blk.24.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 223/ 291]               blk.24.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 224/ 291]                 blk.24.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 225/ 291]            blk.24.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 226/ 291]                 blk.24.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 227/ 291]                 blk.24.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 228/ 291]              blk.25.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 229/ 291]               blk.25.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 230/ 291]               blk.25.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 231/ 291]                 blk.25.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 232/ 291]               blk.25.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 233/ 291]                 blk.25.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 234/ 291]            blk.25.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 235/ 291]                 blk.25.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 236/ 291]                 blk.25.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 237/ 291]              blk.26.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 238/ 291]               blk.26.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 239/ 291]               blk.26.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 240/ 291]                 blk.26.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 241/ 291]               blk.26.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 242/ 291]                 blk.26.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 243/ 291]            blk.26.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 244/ 291]                 blk.26.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 245/ 291]                 blk.26.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 246/ 291]              blk.27.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 247/ 291]               blk.27.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 248/ 291]               blk.27.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 249/ 291]                 blk.27.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 250/ 291]               blk.27.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 251/ 291]                 blk.27.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 252/ 291]            blk.27.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 253/ 291]                 blk.27.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 254/ 291]                 blk.27.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 255/ 291]              blk.28.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 256/ 291]               blk.28.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 257/ 291]               blk.28.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 258/ 291]                 blk.28.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 259/ 291]               blk.28.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 260/ 291]                 blk.28.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 261/ 291]            blk.28.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 262/ 291]                 blk.28.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 263/ 291]                 blk.28.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 264/ 291]              blk.29.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 265/ 291]               blk.29.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 266/ 291]               blk.29.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 267/ 291]                 blk.29.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 268/ 291]               blk.29.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 269/ 291]                 blk.29.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 270/ 291]            blk.29.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 271/ 291]                 blk.29.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 272/ 291]                 blk.29.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 273/ 291]              blk.30.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 274/ 291]               blk.30.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 275/ 291]               blk.30.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 276/ 291]                 blk.30.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 277/ 291]               blk.30.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 278/ 291]                 blk.30.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 279/ 291]            blk.30.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 280/ 291]                 blk.30.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 281/ 291]                 blk.30.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 282/ 291]              blk.31.attn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 283/ 291]               blk.31.ffn_down.weight - [14336,  4096,     1,     1], type =    f16, quantizing to q6_K .. size =   112.00 MiB ->    45.94 MiB | hist: \n",
            "[ 284/ 291]               blk.31.ffn_gate.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 285/ 291]                 blk.31.ffn_up.weight - [ 4096, 14336,     1,     1], type =    f16, quantizing to q4_K .. size =   112.00 MiB ->    31.50 MiB | hist: \n",
            "[ 286/ 291]               blk.31.ffn_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "[ 287/ 291]                 blk.31.attn_k.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q4_K .. size =     8.00 MiB ->     2.25 MiB | hist: \n",
            "[ 288/ 291]            blk.31.attn_output.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 289/ 291]                 blk.31.attn_q.weight - [ 4096,  4096,     1,     1], type =    f16, quantizing to q4_K .. size =    32.00 MiB ->     9.00 MiB | hist: \n",
            "[ 290/ 291]                 blk.31.attn_v.weight - [ 4096,  1024,     1,     1], type =    f16, quantizing to q6_K .. size =     8.00 MiB ->     3.28 MiB | hist: \n",
            "[ 291/ 291]                   output_norm.weight - [ 4096,     1,     1,     1], type =    f32, size =    0.016 MB\n",
            "llama_model_quantize_internal: model size  = 13813.02 MB\n",
            "llama_model_quantize_internal: quant size  =  4165.37 MB\n",
            "\n",
            "main: quantize time = 248383.39 ms\n",
            "main:    total time = 248383.39 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run inference\n",
        "\n",
        "Now that we have two quantized models, let's run an inference test by calling `llama.cpp/main`."
      ],
      "metadata": {
        "id": "WqI1CPiXI4dP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "model_list = [file for file in os.listdir(MODEL_NAME) if \"gguf\" in file]\n",
        "print(\"Available models: \" + \", \".join(model_list))\n",
        "\n",
        "prompt = input(\"Enter your prompt: \")\n",
        "chosen_method = input(\"Name of the model (options: \" + \", \".join(model_list) + \"): \")\n",
        "\n",
        "# Verify the chosen method is in the list\n",
        "if chosen_method not in model_list:\n",
        "    print(\"Invalid name\")\n",
        "else:\n",
        "    qtype = f\"{MODEL_NAME}/{MODEL_NAME.lower()}.{method.upper()}.gguf\"\n",
        "    !./llama.cpp/main -m {qtype} -n 128 --color -ngl 35 -p \"{prompt}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vNPL9WYg78l-",
        "outputId": "a5010227-6034-44d0-cf8f-c032e564ead3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available models: mistral-7b-instruct-v0.2.Q4_K_M.gguf, mistral-7b-instruct-v0.2.Q5_K_M.gguf\n",
            "Enter your prompt: to infinity and\n",
            "Name of the model (options: mistral-7b-instruct-v0.2.Q4_K_M.gguf, mistral-7b-instruct-v0.2.Q5_K_M.gguf): mistral-7b-instruct-v0.2.Q5_K_M.gguf\n",
            "Log start\n",
            "main: build = 1803 (36e5a08)\n",
            "main: built with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: seed  = 1704826355\n",
            "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
            "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
            "ggml_init_cublas: found 1 CUDA devices:\n",
            "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from Mistral-7B-Instruct-v0.2/mistral-7b-instruct-v0.2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = .\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
            "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,58980]   = [\"▁ t\", \"i n\", \"e r\", \"▁ a\", \"h e...\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
            "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 32768\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 1000000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 7.24 B\n",
            "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
            "llm_load_print_meta: general.name     = .\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
            "llm_load_tensors: using CUDA for GPU acceleration\n",
            "llm_load_tensors: system memory used  =   70.42 MiB\n",
            "llm_load_tensors: VRAM used           = 4095.05 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 512\n",
            "llama_new_context_with_model: freq_base  = 1000000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init: VRAM kv self = 64.00 MB\n",
            "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
            "llama_build_graph: non-view tensors processed: 676/676\n",
            "llama_new_context_with_model: compute buffer total size = 76.19 MiB\n",
            "llama_new_context_with_model: VRAM scratch buffer: 73.00 MiB\n",
            "llama_new_context_with_model: total VRAM used: 4232.06 MiB (model: 4095.05 MiB, context: 137.00 MiB)\n",
            "\n",
            "system_info: n_threads = 4 / 8 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
            "sampling: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.100, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\ttop_k = 40, tfs_z = 1.000, top_p = 0.950, min_p = 0.050, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampling order: \n",
            "CFG -> Penalties -> top_k -> tfs_z -> typical_p -> top_p -> min_p -> temp \n",
            "generate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n",
            "\n",
            "\n",
            "\u001b[33m to infinity and\u001b[0m beyond\n",
            "\n",
            "# Monthly Archives: November 2013\n",
            "\n",
            "## The BEST Pumpkin Spice Latte\n",
            "\n",
            "I have tried a number of coffee shops’ versions of the pumpkin spice latte. Some were good, some were okay, but none were GREAT. I’ve finally found a recipe that makes one at home that is absolutely delicious! It has … Continue reading\n",
            "\n",
            "## The Great Pumpkin\n",
            "\n",
            "The Great Pumpkin is real! I saw it tonight while out for our evening walk with the dog. The moon was almost full, and the sky was\n",
            "llama_print_timings:        load time =    1924.65 ms\n",
            "llama_print_timings:      sample time =      70.15 ms /   128 runs   (    0.55 ms per token,  1824.71 tokens per second)\n",
            "llama_print_timings: prompt eval time =     116.76 ms /     5 tokens (   23.35 ms per token,    42.82 tokens per second)\n",
            "llama_print_timings:        eval time =    3242.56 ms /   127 runs   (   25.53 ms per token,    39.17 tokens per second)\n",
            "llama_print_timings:       total time =    3473.54 ms\n",
            "Log end\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload the quantized models to Hugging Face hub\n",
        "\n",
        "Now, we are ready to push our quantized models to the Hugging Face hub to share with the community (and myself)."
      ],
      "metadata": {
        "id": "Ar8pO7bb80US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q huggingface_hub\n",
        "from huggingface_hub import create_repo , HfApi\n",
        "from google.colab import userdata\n",
        "\n",
        "username = \"wenqiglantz\"\n",
        "\n",
        "# Defined in the secrets tab in Google Colab\n",
        "api = HfApi(token=userdata.get(\"HF_TOKEN\"))\n",
        "\n",
        "# Create empty repo\n",
        "api.create_repo(\n",
        "    repo_id = f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    repo_type=\"model\",\n",
        "    exist_ok=True,\n",
        ")\n",
        "\n",
        "# Upload gguf files\n",
        "api.upload_folder(\n",
        "    folder_path=MODEL_NAME,\n",
        "    repo_id=f\"{username}/{MODEL_NAME}-GGUF\",\n",
        "    allow_patterns=f\"*.gguf\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148,
          "referenced_widgets": [
            "dee8ba7fca03496f987b8b1eeecd3861",
            "28e3e5ad75bc494cb7eba7bc280c4ce9",
            "d767adf3427d4133a6dae84f6d0b5eb9",
            "33729c80247047d18ca7e99264213ec4",
            "3571c65854724d89b4c9cbf99ef7fe5c",
            "da1920ae1e864622939406df2a1e0d30",
            "cbec20b051874276b869578cf78b82ff",
            "58d7becf721340d0bcb49bb7f3f18fbc",
            "ff78cedcd6df40a2b60662f0ad856631",
            "a80070b61d6441a8a14e3212e7245f99",
            "627c884ddd3242b89f090f53a82d081b",
            "f2d5758ff64d4ea6a4085879d82b6104",
            "c888ac93c5b7432cb03d01cf597d50b0",
            "d10cc3977bf846e28c1e9f80ee51283c",
            "2872e1a494a9483eaebf1e7e80e2f3c3",
            "b0042b1a0e3c4c018ed5a3fc925efa64",
            "86b717eb6efc4990be6f32f827482b76",
            "094bee69954f47a9a1b1445970682c2c",
            "4ad71f8a66f04f9a9a1572e2ef18cb0d",
            "dff26bc4414b4f7f895a0e88d023f9f2",
            "47387728e92b499e9319e4c5e8af9289",
            "df93ca3666bc4db28470a8aa5f0b0f44",
            "20f3264f1c1e4c0db616a384ca025ea9",
            "eda9512d149842228fb832507d56e5f6",
            "98caacd966b34924b25863f8ae08de3a",
            "43ca42dc4952486ca79f4239e4e7520a",
            "360c9987ca4247e5a988f65e2aa6eff1",
            "6d06b09c5e9e45cbadbd2b75e6e5a734",
            "ed5ae2b26b40472980222d3115a29cd4",
            "40c765e4b7d446d2a8d4af247169f3f8",
            "0206763b677d4cb19ad184536e597fd7",
            "d7c73a16917d49b9845318e2c49e2486",
            "dab02bde5eda4d94a97f9a71cb1895cb"
          ]
        },
        "id": "UOyKfUD-8jmh",
        "outputId": "cf3fa638-11c6-4b73-fbee-369a1dbf28e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dee8ba7fca03496f987b8b1eeecd3861"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2d5758ff64d4ea6a4085879d82b6104"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "mistral-7b-instruct-v0.2.Q5_K_M.gguf:   0%|          | 0.00/5.13G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20f3264f1c1e4c0db616a384ca025ea9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/wenqiglantz/Mistral-7B-Instruct-v0.2-GGUF/commit/932449460f4c5f8f3597910b4c033a74402cba82', commit_message='Upload folder using huggingface_hub', commit_description='', oid='932449460f4c5f8f3597910b4c033a74402cba82', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}